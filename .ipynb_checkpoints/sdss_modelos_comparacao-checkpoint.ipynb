{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titulo\n",
    "## Subtitulo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Importando bibliotecas essenciais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Exibindo mensagens informativas para cada passo\n",
    "print(\"Bibliotecas importadas com sucesso!\")\n",
    "\n",
    "# Lendo o arquivo CSV (ajustando o separador se necessário)\n",
    "file_path = 'sdss.csv'  # Substitua pelo caminho correto se estiver em outro ambiente\n",
    "df = pd.read_csv(file_path, skiprows=1)\n",
    "# Mantendo apenas as primeiras 50 mil linhas\n",
    "#df = df.head(200000)\n",
    "\n",
    "# Exibindo as 5 primeiras linhas para verificar se os dados foram carregados corretamente\n",
    "print(\"Dados carregados com sucesso! Primeiras 5 linhas:\")\n",
    "print(df.head())\n",
    "\n",
    "# Exibindo o tamanho do dataset\n",
    "print(f\"O dataset contém {df.shape[0]} linhas e {df.shape[1]} colunas.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exibindo informações gerais sobre o dataset\n",
    "print(\"\\nInformações gerais do dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "# Exibindo a contagem de valores únicos para a coluna de classificação\n",
    "print(\"\\nDistribuição das classes (quantidade de estrelas, galáxias e quasares):\")\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# Plotando a distribuição das classes\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x='class', order=df['class'].value_counts().index, palette='viridis')\n",
    "plt.title(\"Distribuição das Classes\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Quantidade\")\n",
    "plt.show()\n",
    "\n",
    "# Selecionando algumas features principais para análise (por exemplo, 'u', 'g', 'r', 'i', 'z')\n",
    "features = ['u', 'g', 'r', 'i', 'z','redshift']\n",
    "\n",
    "# Substituindo valores infinitos por NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Removendo linhas com valores NaN (se necessário)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Gerando histogramas para as principais features\n",
    "print(\"\\nGerando histogramas para as principais features...\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(features, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.histplot(df[feature], kde=True, bins=30, color='blue')\n",
    "    plt.title(f\"Histograma de {feature}\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Frequência\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Análise dos dados concluída com sucesso!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Substituindo valores infinitos por NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Removendo linhas com valores NaN\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Método IQR para remover outliers\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    for feature in columns:\n",
    "        Q1 = df[feature].quantile(0.25)\n",
    "        Q3 = df[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        print(f\"\\nOutliers na feature '{feature}' usando IQR:\")\n",
    "        print(df[(df[feature] < lower_bound) | (df[feature] > upper_bound)])\n",
    "\n",
    "        # Remover os outliers\n",
    "        df = df[(df[feature] >= lower_bound) & (df[feature] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "# Método Z-Score para remover outliers\n",
    "def remove_outliers_zscore(df, columns, threshold=3.0):\n",
    "    z_scores = df[columns].apply(zscore)\n",
    "    outliers = (z_scores.abs() > threshold).any(axis=1)\n",
    "\n",
    "    print(\"\\nLinhas com outliers usando Z-Score:\")\n",
    "    print(df[outliers])\n",
    "\n",
    "    # Remover outliers\n",
    "    df = df[~outliers]\n",
    "    return df\n",
    "\n",
    "# Visualizar boxplots antes da remoção\n",
    "def plot_boxplots(df, columns, title=\"Boxplots Antes da Remoção de Outliers\"):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, feature in enumerate(columns):\n",
    "        plt.subplot(1, len(columns), i + 1)\n",
    "        sns.boxplot(data=df, x=feature, color='cyan')\n",
    "        plt.title(f\"Boxplot: {feature}\")\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Colunas para análise\n",
    "columns = ['i', 'z']\n",
    "\n",
    "# Visualizar boxplots antes da remoção de outliers\n",
    "plot_boxplots(df, columns, title=\"Boxplots Antes da Remoção de Outliers\")\n",
    "\n",
    "# Removendo outliers usando IQR\n",
    "df = remove_outliers_iqr(df, columns)\n",
    "\n",
    "# Visualizar boxplots após remoção usando IQR\n",
    "plot_boxplots(df, columns, title=\"Boxplots Após Remoção de Outliers (IQR)\")\n",
    "\n",
    "# Removendo outliers usando Z-Score (se necessário)\n",
    "df = remove_outliers_zscore(df, columns)\n",
    "\n",
    "# Visualizar boxplots após remoção usando Z-Score\n",
    "plot_boxplots(df, columns, title=\"Boxplots Após Remoção de Outliers (Z-Score)\")\n",
    "\n",
    "# Verificando informações gerais do dataset após remoção\n",
    "print(\"\\nInformações gerais do dataset após remoção de outliers:\")\n",
    "print(df.info())\n",
    "\n",
    "# Gerando histogramas para as principais features\n",
    "print(\"\\nGerando histogramas para as principais features...\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(columns, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.histplot(df[feature], kde=True, bins=30, color='blue')\n",
    "    plt.title(f\"Histograma de {feature}\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Frequência\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Análise e remoção de outliers concluídas com sucesso!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1. Verificando valores nulos ou em branco\n",
    "print(\"\\nVerificando valores nulos no dataset:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Se existirem valores nulos, vamos removê-los\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    print(\"\\nExistem valores nulos. Removendo essas linhas...\")\n",
    "    df.dropna(inplace=True)\n",
    "    print(\"Valores nulos removidos.\")\n",
    "\n",
    "# 2. Guardando os nomes das colunas\n",
    "colunas_originais = df.columns.tolist()\n",
    "print(\"\\nColunas originais guardadas para referência futura.\")\n",
    "\n",
    "# 3. Verificando e mapeando os labels (encoding das classes)\n",
    "print(\"\\nClasses únicas no dataset:\")\n",
    "print(df['class'].unique())\n",
    "\n",
    "# Mapear as classes para valores numéricos\n",
    "class_mapping = {'STAR': 0, 'GALAXY': 1, 'QSO': 2}\n",
    "df['class'] = df['class'].map(class_mapping)\n",
    "\n",
    "print(\"\\nClasses mapeadas para valores numéricos:\")\n",
    "print(class_mapping)\n",
    "\n",
    "# 4. Dropar dados desnecessários (Exemplo: colunas irrelevantes)\n",
    "# Aqui você pode ajustar conforme o caso (e.g., IDs únicos que não ajudam na classificação)\n",
    "columns_to_drop = ['objid', 'specobjid']  # Exemplos de colunas desnecessárias\n",
    "#columns_to_drop = ['objid', 'specobjid', 'run', 'rerun', 'camcol', 'field', 'plate', 'mjd', 'fiberid', 'ra', 'dec']\n",
    "df.drop(columns=columns_to_drop, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "print(f\"\\nColunas desnecessárias removidas: {columns_to_drop}\")\n",
    "\n",
    "# 5. Normalizando os dados\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features = df.drop('class', axis=1)  # Excluindo a coluna alvo para normalizar apenas as features\n",
    "target = df['class']\n",
    "\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "print(\"\\nNormalização das features concluída.\")\n",
    "\n",
    "# Atualizando o DataFrame com os dados normalizados\n",
    "df_scaled = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "df_scaled['class'] = target.values\n",
    "\n",
    "print(\"\\nDataset preparado com sucesso!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separando features (X) e target (y)\n",
    "X = df_scaled.drop('class', axis=1)\n",
    "y = df_scaled['class']\n",
    "\n",
    "# Dividindo os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "# Exibindo as dimensões dos conjuntos\n",
    "print(f\"Conjunto de treinamento: {X_train.shape[0]} amostras, {X_train.shape[1]} features\")\n",
    "print(f\"Conjunto de teste: {X_test.shape[0]} amostras, {X_test.shape[1]} features\")\n",
    "\n",
    "print(\"\\nSplit de treino e teste realizado com sucesso!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Dicionário para armazenar os resultados\n",
    "resultados_modelos = []\n",
    "\n",
    "# Função genérica para treinar e avaliar modelos\n",
    "def avaliar_modelo(nome_modelo, modelo, X_train, X_test, y_train, y_test, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Avalia um modelo nos dados de treino e teste, incluindo validação cruzada.\n",
    "    \n",
    "    Parâmetros:\n",
    "        nome_modelo (str): Nome do modelo a ser avaliado.\n",
    "        modelo: Instância do modelo a ser treinado.\n",
    "        X_train, X_test, y_train, y_test: Dados de treino e teste.\n",
    "        cv_folds (int): Número de folds para validação cruzada.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTreinando o modelo: {nome_modelo}...\")\n",
    "    \n",
    "    # Medindo o tempo de treinamento\n",
    "    inicio_treino = time.time()\n",
    "    modelo.fit(X_train, y_train)\n",
    "    fim_treino = time.time()\n",
    "\n",
    "    # Fazendo predições\n",
    "    inicio_pred = time.time()\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    fim_pred = time.time()\n",
    "\n",
    "    # Calculando probabilidades para ROC-AUC (se o modelo suportar)\n",
    "    try:\n",
    "        if len(modelo.classes_) > 2:  # Caso seja multiclasse\n",
    "            y_pred_proba = modelo.predict_proba(X_test)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class=\"ovr\")\n",
    "        else:  # Caso seja binário\n",
    "            y_pred_proba = modelo.predict_proba(X_test)[:, 1]\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        roc_auc = roc_auc * 100  # Convertendo para percentual\n",
    "    except AttributeError:\n",
    "        roc_auc = \"N/A\"\n",
    "\n",
    "    # Calculando métricas\n",
    "    acuracia = accuracy_score(y_test, y_pred) * 100  # Convertendo para percentual\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted') * 100  # Convertendo para percentual\n",
    "\n",
    "    # Validação cruzada\n",
    "    try:\n",
    "        cross_val_scores = cross_val_score(modelo, X_train, y_train, cv=cv_folds, scoring='accuracy')\n",
    "        cross_val_mean = cross_val_scores.mean() * 100  # Média da acurácia\n",
    "        cross_val_std = cross_val_scores.std() * 100   # Desvio padrão da acurácia\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao realizar validação cruzada para {nome_modelo}: {e}\")\n",
    "        cross_val_mean, cross_val_std = \"N/A\", \"N/A\"\n",
    "\n",
    "    # Calculando matriz de confusão\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Salvando resultados no dicionário\n",
    "    resultados_modelos.append({\n",
    "        \"Modelo\": nome_modelo,\n",
    "        \"Acurácia (%)\": round(acuracia, 2),\n",
    "        \"F1-Score (%)\": round(f1, 2),\n",
    "        \"ROC-AUC (%)\": round(roc_auc, 2) if roc_auc != \"N/A\" else \"N/A\",\n",
    "        \"Tempo de Treinamento (s)\": round(fim_treino - inicio_treino, 2),\n",
    "        \"Tempo de Predição (s)\": round(fim_pred - inicio_pred, 2),\n",
    "        \"Validação Cruzada - Média (%)\": round(cross_val_mean, 2) if cross_val_mean != \"N/A\" else \"N/A\",\n",
    "        \"Validação Cruzada - Stdev (%)\": round(cross_val_std, 2) if cross_val_std != \"N/A\" else \"N/A\",\n",
    "        \"Matriz Confusão\": cm,  # Matriz de Confusão Absoluta\n",
    "        \"Predições\": y_pred,    # Predições do Modelo\n",
    "        \"Reais\": y_test,        # Valores Reais\n",
    "        \"Classes\": modelo.classes_ if hasattr(modelo, \"classes_\") else None  # Classes do Modelo\n",
    "    })\n",
    "\n",
    "    print(f\"Modelo {nome_modelo} avaliado com sucesso!\")\n",
    "    print(f\"Acurácia: {acuracia:.2f}%, F1-Score: {f1:.2f}%, ROC-AUC: {roc_auc if roc_auc != 'N/A' else 'N/A'}\\n\")\n",
    "    if cross_val_mean != \"N/A\":\n",
    "        print(f\"Validação Cruzada - Média: {cross_val_mean:.2f}%, Desvio Padrão: {cross_val_std:.2f}%\\n\")\n",
    "\n",
    "    # Matriz de Confusão (Percentual) com Nomes Decodificados\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    decoded_classes = [k for k, v in sorted(class_mapping.items(), key=lambda item: item[1])]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_percent, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=decoded_classes, yticklabels=decoded_classes)\n",
    "    plt.title(f\"Matriz de Confusão (Percentual) - {nome_modelo}\")\n",
    "    plt.xlabel(\"Predito\")\n",
    "    plt.ylabel(\"Real\")\n",
    "    plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Inicializando o modelo Decision Tree\n",
    "modelo_dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Avaliando o modelo\n",
    "avaliar_modelo(\"Decision Tree\", modelo_dt, X_train, X_test, y_train, y_test)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Inicializando o modelo Random Forest\n",
    "modelo_rf = RandomForestClassifier(random_state=1, n_estimators=100)\n",
    "\n",
    "# Avaliando o modelo\n",
    "avaliar_modelo(\"Random Forest\", modelo_rf, X_train, X_test, y_train, y_test)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Inicializando o modelo Gradient Boosting\n",
    "modelo_gb = GradientBoostingClassifier(random_state=1, n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "# Avaliando o modelo\n",
    "avaliar_modelo(\"Gradient Boosting\", modelo_gb, X_train, X_test, y_train, y_test)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Inicializando o modelo Logistic Regression\n",
    "modelo_lr = LogisticRegression(random_state=1, max_iter=1000)\n",
    "\n",
    "# Avaliando o modelo\n",
    "avaliar_modelo(\"Logistic Regression\", modelo_lr, X_train, X_test, y_train, y_test)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Inicializando o modelo Naive Bayes (Gaussian)\n",
    "modelo_nb = GaussianNB()\n",
    "\n",
    "# Avaliando o modelo\n",
    "avaliar_modelo(\"Naive Bayes\", modelo_nb, X_train, X_test, y_train, y_test)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Inicializando o modelo MLPClassifier\n",
    "modelo_mlp = MLPClassifier(hidden_layer_sizes=(60,), max_iter=300, random_state=1)\n",
    "\n",
    "# Avaliando o modelo\n",
    "avaliar_modelo(\"Rede Neural Rasa (MLP)\", modelo_mlp, X_train, X_test, y_train, y_test)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# Inicializando o modelo SVM\n",
    "# modelo_svm = SVC(kernel='rbf', random_state=1)\n",
    "\n",
    "# Avaliando o modelo\n",
    "# avaliar_modelo(False,\"Support Vector Machine (SVM)\", modelo_svm, X_train, X_test, y_train, y_test)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Criando o pipeline com StandardScaler e LinearSVC\n",
    "modelo_svm = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Normalização dos dados\n",
    "    ('svc', LinearSVC(random_state=1, max_iter=1000))  # Modelo LinearSVC\n",
    "])\n",
    "\n",
    "# Avaliando o modelo\n",
    "avaliar_modelo(\"Support Vector Machine (Linear SVC)\", modelo_svm, X_train, X_test, y_train, y_test)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Inicializando o modelo Extra Trees\n",
    "modelo_et = ExtraTreesClassifier(random_state=1)\n",
    "\n",
    "# Avaliando o modelo\n",
    "avaliar_modelo(\"Extra Trees Classifier\", modelo_et, X_train, X_test, y_train, y_test)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Inicializando o modelo SGDClassifier\n",
    "modelo_sgd = SGDClassifier(loss='hinge', random_state=1)\n",
    "\n",
    "# Avaliando o modelo\n",
    "avaliar_modelo(\"Stochastic Gradient Descent (SGD)\", modelo_sgd, X_train, X_test, y_train, y_test)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Inicializando o modelo KNN\n",
    "modelo_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Avaliando o modelo\n",
    "avaliar_modelo(\"K-Nearest Neighbors (KNN)\", modelo_knn, X_train, X_test, y_train, y_test)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Inicializando o modelo AdaBoost\n",
    "modelo_ab = AdaBoostClassifier(random_state=1, n_estimators=50)\n",
    "\n",
    "# Avaliando o modelo\n",
    "avaliar_modelo(\"AdaBoost Classifier\", modelo_ab, X_train, X_test, y_train, y_test)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Consolidando os resultados em um DataFrame\n",
    "resultados_df = pd.DataFrame(resultados_modelos)\n",
    "\n",
    "# Ordenando os resultados pela Acurácia\n",
    "resultados_df.sort_values(by=\"Acurácia (%)\", ascending=False, inplace=True)\n",
    "\n",
    "# Convertendo colunas numéricas para o formato correto\n",
    "cols_to_convert = [\"Acurácia (%)\", \"F1-Score (%)\", \"ROC-AUC (%)\", \"Tempo de Treinamento (s)\", \"Tempo de Predição (s)\", \"Validação Cruzada - Média (%)\", \"Validação Cruzada - Stdev (%)\"]\n",
    "for col in cols_to_convert:\n",
    "    resultados_df[col] = pd.to_numeric(resultados_df[col], errors='coerce')\n",
    "\n",
    "# Dividindo os dados para melhorar a visualização\n",
    "heatmap_data_metrics = resultados_df[[\"Modelo\", \"Acurácia (%)\", \"F1-Score (%)\", \"ROC-AUC (%)\"]]\n",
    "heatmap_data_times = resultados_df[[\"Modelo\", \"Tempo de Treinamento (s)\", \"Tempo de Predição (s)\", \"Validação Cruzada - Média (%)\", \"Validação Cruzada - Stdev (%)\"]]\n",
    "\n",
    "# Função para criar tabela visual\n",
    "def plot_table(data, title, col_palettes):\n",
    "    # Cálculo do range para cada coluna\n",
    "    col_ranges = {\n",
    "        col: (data[col].min(), data[col].max())\n",
    "        for col in data.columns if data[col].dtype in [float, int]\n",
    "    }\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.axis('off')  # Remover os eixos\n",
    "\n",
    "    # Adicionando a tabela ao gráfico\n",
    "    table = ax.table(\n",
    "        cellText=data.values,\n",
    "        colLabels=data.columns,\n",
    "        loc='center',\n",
    "        cellLoc='center',\n",
    "        colLoc='center'\n",
    "    )\n",
    "\n",
    "    # Ajustando estilo da tabela\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.auto_set_column_width(col=list(range(len(data.columns))))\n",
    "\n",
    "    # Aplicando gradiente para cada célula numérica\n",
    "    for (row, col), cell in table._cells.items():\n",
    "        if row > 0 and col > 0:  # Apenas células de métricas (não incluir \"Modelo\")\n",
    "            column_name = data.columns[col]\n",
    "            if column_name in col_ranges:  # Garantir que é uma métrica numérica\n",
    "                value = data.iloc[row - 1, col]\n",
    "                min_val, max_val = col_ranges[column_name]\n",
    "                normalized_value = (value - min_val) / (max_val - min_val) if max_val > min_val else 0.5\n",
    "                cell_color = col_palettes.get(column_name, plt.cm.Blues)(normalized_value)\n",
    "\n",
    "                # Aplicar cor de fundo e ajustar cor da fonte\n",
    "                cell.set_facecolor(cell_color)\n",
    "                text_color = \"white\" if normalized_value > 0.6 else \"black\"\n",
    "                cell.get_text().set_color(text_color)\n",
    "        else:\n",
    "            cell.set_facecolor('white')  # Fundo branco para cabeçalhos\n",
    "\n",
    "    plt.title(title, fontsize=14, pad=10)\n",
    "    plt.tight_layout(pad=1)\n",
    "    plt.show()\n",
    "\n",
    "# Definindo paletas de cores específicas para cada métrica\n",
    "col_palettes_metrics = {\n",
    "    \"Acurácia (%)\": plt.cm.Blues,\n",
    "    \"F1-Score (%)\": plt.cm.Oranges,\n",
    "    \"ROC-AUC (%)\": plt.cm.Greens,\n",
    "}\n",
    "\n",
    "col_palettes_times = {\n",
    "    \"Tempo de Treinamento (s)\": plt.cm.Reds,\n",
    "    \"Tempo de Predição (s)\": plt.cm.Purples,\n",
    "    \"Validação Cruzada - Média (%)\": plt.cm.Greens,\n",
    "    \"Validação Cruzada - Stdev (%)\": plt.cm.Greys,\n",
    "}\n",
    "\n",
    "# Plotando as tabelas\n",
    "print(\"\\nTabela de Métricas de Desempenho:\")\n",
    "plot_table(heatmap_data_metrics, \"Métricas de Desempenho dos Modelos\", col_palettes_metrics)\n",
    "\n",
    "print(\"\\nTabela de Tempos e Validação Cruzada:\")\n",
    "plot_table(heatmap_data_times, \"Tempos e Validação Cruzada dos Modelos\", col_palettes_times)\n",
    "\n",
    "\n",
    "\n",
    "# Gráfico 1: Comparação de Acurácia\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(x=\"Acurácia (%)\", y=\"Modelo\", data=resultados_df, palette=\"Blues_d\")\n",
    "plt.title(\"Comparação de Acurácia dos Modelos\")\n",
    "plt.xlabel(\"Acurácia (%)\")\n",
    "plt.ylabel(\"Modelos\")\n",
    "plt.xlim(0, resultados_df[\"Acurácia (%)\"].max() + 10)\n",
    "\n",
    "# Adicionando os valores nas barras\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.2f%%\")\n",
    "plt.show()\n",
    "\n",
    "# Gráfico 2: Comparação de F1-Score\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(x=\"F1-Score (%)\", y=\"Modelo\", data=resultados_df, palette=\"Oranges_d\")\n",
    "plt.title(\"Comparação de F1-Score dos Modelos\")\n",
    "plt.xlabel(\"F1-Score (%)\")\n",
    "plt.ylabel(\"Modelos\")\n",
    "plt.xlim(0, resultados_df[\"F1-Score (%)\"].max() + 10)\n",
    "\n",
    "# Adicionando os valores nas barras\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.2f%%\")\n",
    "plt.show()\n",
    "\n",
    "# Gráfico 3: Comparação de Tempo de Treinamento\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(x=\"Tempo de Treinamento (s)\", y=\"Modelo\", data=resultados_df, palette=\"Greens_d\")\n",
    "plt.title(\"Comparação de Tempo de Treinamento dos Modelos\")\n",
    "plt.xlabel(\"Tempo de Treinamento (s)\")\n",
    "plt.ylabel(\"Modelos\")\n",
    "plt.xlim(0, resultados_df[\"Tempo de Treinamento (s)\"].max() + 50)\n",
    "\n",
    "# Adicionando os valores nas barras\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.2f s\")\n",
    "plt.show()\n",
    "\n",
    "# Gerar um dicionário de cores único baseado nos modelos\n",
    "palette = sns.color_palette(\"husl\", n_colors=len(resultados_df[\"Modelo\"]))\n",
    "model_colors = dict(zip(resultados_df[\"Modelo\"], palette))\n",
    "\n",
    "# Gráfico 4: Scatter Plot de Acurácia vs. Tempo de Treinamento\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=\"Tempo de Treinamento (s)\", y=\"Acurácia (%)\", hue=\"Modelo\", data=resultados_df, palette=model_colors, s=350)\n",
    "plt.title(\"Relação entre Acurácia e Tempo de Treinamento\")\n",
    "plt.xlabel(\"Tempo de Treinamento (s)\")\n",
    "plt.ylabel(\"Acurácia (%)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Gráfico 5: Scatter Plot de F1-Score vs. Tempo de Predição\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=\"Tempo de Predição (s)\", y=\"F1-Score (%)\", hue=\"Modelo\", data=resultados_df, palette=model_colors, s=350, edgecolor='w', linewidth=1)  \n",
    "plt.title(\"Relação entre F1-Score e Tempo de Predição\")\n",
    "plt.xlabel(\"Tempo de Predição (s)\")\n",
    "plt.ylabel(\"F1-Score (%)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ordenando os modelos por Acurácia Teste\n",
    "resultados_df = resultados_df.sort_values(by=\"Acurácia (%)\", ascending=False)\n",
    "\n",
    "# Dados atualizados\n",
    "modelos = resultados_df[\"Modelo\"]\n",
    "acc_test = resultados_df[\"Acurácia (%)\"]\n",
    "acc_val_mean = resultados_df[\"Validação Cruzada - Média (%)\"]\n",
    "acc_val_std = resultados_df[\"Validação Cruzada - Stdev (%)\"]\n",
    "\n",
    "x = np.arange(len(modelos))  # Posição dos modelos\n",
    "width = 0.4  # Largura das barras\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Barras de teste\n",
    "ax.bar(x - width/2, acc_test, width, label=\"Acurácia Teste\", color=\"blue\")\n",
    "\n",
    "# Barras de validação cruzada\n",
    "ax.bar(x + width/2, acc_val_mean, width, yerr=acc_val_std, label=\"Acurácia Validação Cruzada\", color=\"orange\", capsize=5)\n",
    "\n",
    "# Adicionando valores nas barras\n",
    "\"\"\" \n",
    "for i, v in enumerate(acc_test):\n",
    "    ax.text(i - 0.5, v + 0.5, f\"{v:.1f}%\", color=\"blue\", ha=\"center\", fontsize=10)\n",
    "\n",
    "for i, v in enumerate(acc_val_mean):\n",
    "    ax.text(i + 0.5, v + 0.5, f\"{v:.1f}%\", color=\"orange\", ha=\"center\", fontsize=10)\n",
    "\"\"\"\n",
    "\n",
    "# Personalização do gráfico\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(modelos, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Acurácia (%)\")\n",
    "ax.set_title(\"Comparação de Acurácia: Teste vs Validação Cruzada\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "# Decodificando as classes\n",
    "class_mapping = {0: 'STAR', 1: 'GALAXY', 2: 'QSO'}\n",
    "class_names = list(class_mapping.values())\n",
    "\n",
    "# 1. Heatmap de Erros por Classe e Modelo\n",
    "def plot_heatmap(resultados_modelos):\n",
    "    num_classes = len(class_names)\n",
    "    erros_por_modelo = []\n",
    "\n",
    "    for modelo in resultados_modelos:\n",
    "        cm = modelo[\"Matriz Confusão\"]\n",
    "        erros = [sum(cm[classe]) - cm[classe][classe] for classe in range(num_classes)]\n",
    "        erros_por_modelo.append(erros)\n",
    "\n",
    "    erros_df = pd.DataFrame(erros_por_modelo, columns=class_names)\n",
    "    erros_df[\"Modelo\"] = [modelo[\"Modelo\"] for modelo in resultados_modelos]\n",
    "    erros_df.set_index(\"Modelo\", inplace=True)\n",
    "\n",
    "    # Plotando o Heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(erros_df, annot=True, fmt=\"d\", cmap=\"Reds\", xticklabels=class_names, yticklabels=erros_df.index)\n",
    "    plt.title(\"Heatmap de Erros por Modelo e Classe\")\n",
    "    plt.xlabel(\"Classe\")\n",
    "    plt.ylabel(\"Modelo\")\n",
    "    plt.show()\n",
    "\n",
    "# 2. Gráfico de Barras para Acertos, Erros e Validação Cruzada de Cada Classe\n",
    "def plot_barras(resultados_modelos, classe_idx):\n",
    "    # Verificando se a classe existe no mapeamento\n",
    "    classe_name = class_mapping.get(classe_idx)\n",
    "    if not classe_name:\n",
    "        raise ValueError(f\"Classe índice {classe_idx} não encontrada no mapeamento {class_mapping}.\")\n",
    "\n",
    "    resultados_classe = []\n",
    "\n",
    "    for modelo in resultados_modelos:\n",
    "        cm = modelo[\"Matriz Confusão\"]\n",
    "        acertos = cm[classe_idx][classe_idx]\n",
    "        erros = sum(cm[classe_idx]) - acertos\n",
    "        acc_val_mean = modelo.get(\"Validação Cruzada - Média (%)\", 0)\n",
    "        acc_val_std = modelo.get(\"Validação Cruzada - Stdev (%)\", 0)\n",
    "        resultados_classe.append({\n",
    "            \"Modelo\": modelo[\"Modelo\"],\n",
    "            \"Acertos\": acertos,\n",
    "            \"Erros\": erros,\n",
    "            \"Validação Cruzada - Média (%)\": acc_val_mean,\n",
    "            \"Validação Cruzada - Stdev (%)\": acc_val_std\n",
    "        })\n",
    "\n",
    "    resultados_classe_df = pd.DataFrame(resultados_classe)\n",
    "    resultados_classe_df.set_index(\"Modelo\", inplace=True)\n",
    "    \n",
    "    # Gráfico de Barras com dois eixos Y\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    bar_width = 0.4  # Largura das barras\n",
    "    modelos = resultados_classe_df.index\n",
    "    x = np.arange(len(modelos))\n",
    "\n",
    "    # Eixo para Acertos e Erros\n",
    "    ax1.bar(x - bar_width / 2, resultados_classe_df[\"Acertos\"], width=bar_width, label=\"Acertos\", color=\"#377eb8\")\n",
    "    ax1.bar(x - bar_width / 2, resultados_classe_df[\"Erros\"], bottom=resultados_classe_df[\"Acertos\"], width=bar_width, label=\"Erros\", color=\"#e41a1c\")\n",
    "    ax1.set_ylabel(\"Quantidade\", color=\"black\")\n",
    "    ax1.set_xlabel(\"Modelos\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(modelos, rotation=45, ha=\"right\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1.set_title(f\"Comparação de Acertos, Erros e Validação Cruzada - Classe {classe_name}\")\n",
    "\n",
    "    # Segundo eixo Y para Validação Cruzada\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.errorbar(x + bar_width / 2, resultados_classe_df[\"Validação Cruzada - Média (%)\"], \n",
    "                 yerr=resultados_classe_df[\"Validação Cruzada - Stdev (%)\"], \n",
    "                 fmt='o', color=\"#4daf4a\", label=\"Validação Cruzada (Média)\", capsize=5)\n",
    "    ax2.set_ylabel(\"Validação Cruzada (%)\", color=\"#4daf4a\")\n",
    "    ax2.tick_params(axis='y', labelcolor=\"#4daf4a\")\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 3. Curvas de Métricas para Cada Classe\n",
    "def plot_curvas_metricas(resultados_modelos, classe_idx):\n",
    "    classe_name = class_mapping.get(classe_idx)\n",
    "    if not classe_name:\n",
    "        raise ValueError(f\"Classe índice {classe_idx} não encontrada no mapeamento {class_mapping}.\")\n",
    "\n",
    "    recalls, precisions, f1_scores = [], [], []\n",
    "\n",
    "    for modelo in resultados_modelos:\n",
    "        y_pred = modelo[\"Predições\"]\n",
    "        y_test = modelo[\"Reais\"]\n",
    "\n",
    "        recall = recall_score(y_test, y_pred, labels=[classe_idx], average=None)[0]\n",
    "        precision = precision_score(y_test, y_pred, labels=[classe_idx], average=None)[0]\n",
    "        f1 = f1_score(y_test, y_pred, labels=[classe_idx], average=None)[0]\n",
    "\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    metricas_df = pd.DataFrame({\n",
    "        \"Modelo\": [modelo[\"Modelo\"] for modelo in resultados_modelos],\n",
    "        \"Recall\": recalls,\n",
    "        \"Precision\": precisions,\n",
    "        \"F1-Score\": f1_scores\n",
    "    })\n",
    "    metricas_df.set_index(\"Modelo\", inplace=True)\n",
    "\n",
    "    metricas_df.plot(kind=\"bar\", figsize=(12, 6), color=[\"#4E79A7\", \"#F28E2B\", \"#E15759\"])\n",
    "    plt.title(f\"Curvas de Métricas - Classe {classe_name}\")\n",
    "    plt.xlabel(\"Modelos\")\n",
    "    plt.ylabel(\"Valor\")\n",
    "    plt.legend([\"Recall\", \"Precision\", \"F1-Score\"])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Chamando as funções\n",
    "print(\"Heatmap de Erros por Classe e Modelo:\")\n",
    "plot_heatmap(resultados_modelos)\n",
    "\n",
    "print(\"\\nGráficos de Barras para Cada Classe:\")\n",
    "for classe_idx in range(len(class_mapping)):  # Para as classes STAR (0), GALAXY (1), QSO (2)\n",
    "    plot_barras(resultados_modelos, classe_idx)\n",
    "\n",
    "print(\"\\nCurvas de Métricas para Cada Classe:\")\n",
    "for classe_idx in range(len(class_mapping)):  # Para as classes STAR (0), GALAXY (1), QSO (2)\n",
    "    plot_curvas_metricas(resultados_modelos, classe_idx)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
